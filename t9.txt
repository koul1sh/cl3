
This script defines a simple two‐stage MapReduce job using the **mrjob** library to analyze yearly temperature records and determine which year was the coolest and which was the hottest on average. Instead of processing everything in one pass, it breaks the task into two logical phases—first computing per‐year averages, then scanning those averages to find the extremes—mirroring the way large‐scale data pipelines decompose problems for efficiency and scalability.

In the **first stage**, the `mapper_get_temps` function reads each line of input (expected to be `year,temperature`), parses out the year and temperature, and emits them as key–value pairs `(year, temp)`. The framework automatically groups all temperatures by year across the distributed map tasks and feeds them into the `reducer_avg_temp`. This reducer collects the list of temperatures for a single year, computes the average, and emits a unified key `"avg"` with the tuple `(year, average_temp)`—essentially flattening all per‐year results onto a common key so they can be processed together in the next step.

The **second stage** consists only of a reducer, `reducer_find_extremes`, which receives every `(year, average_temp)` pair under the shared key `"avg"`. It scans through them once, tracking the smallest and largest average temperature seen and, at the end, emits a pair identifying the “Coolest Year” and “Hottest Year” along with their temperature values. By pushing the per‐year averages into a single reducer, the code efficiently finds the global extremes without needing to hold all data in memory at once.

Under the hood, **MapReduce** works by executing mappers in parallel to distribute the work of reading and parsing raw data, then shuffling and sorting intermediate pairs by key, and finally running reducers (potentially also in parallel when keys differ). By splitting the logic into two `MRStep`s, this job first parallelizes the per‐year aggregation and then gathers all those aggregates for a centralized comparison. This pattern—**aggregation followed by global analysis**—is extremely common in large‐scale data processing, whether you’re running on Hadoop clusters, AWS EMR, or other distributed engines.

Using **mrjob** lets you write these steps in pure Python without boilerplate Java or Scala; it handles launching mapper and reducer subprocesses, streaming data between them, and even running locally for testing or on Hadoop/YARN in production. In the context of weather analytics, this approach scales to terabytes of sensor logs: each mapper deals with its slice of the log, each reducer averages a particular year’s readings, and a final reducer discovers the record‐setting years. That separation of concerns—local summarization then global synthesis—is the hallmark of MapReduce and is exactly why this two‐step pipeline is both clear to read and powerful in production.
